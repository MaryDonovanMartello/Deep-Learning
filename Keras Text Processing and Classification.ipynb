{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mary Donovan Martello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This file contains code from Deep Learning with Python, www.manning.com/books/deep-learning-with-python, Copyright 2018 Francois Chollet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose:  Transform text input into tokens and convert those tokens into numeric vectors using one-hot encoding and feature hashing.  Build basic text-processing and classification models using recurrent neural networks.  Demonstrate how word embeddings such as Word2Vec can help improve the performance of text-processing models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 10.1:  Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a tokenized function that splits a sentence into words, implement a `ngram` function that splits tokens into N-grams, and implement an one_hot_encode function to create a vector from a numerical vector from a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import unicodedata\n",
    "import sys\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create tokenized function to split a sentence into words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentence and remove punctuation \n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = []\n",
    "   \n",
    "    # tokenize the sentence and remove punctuation\n",
    "    for word in sentence.split():\n",
    "        # lower case the word\n",
    "        word = word.lower()\n",
    "        # For each token, remove any punctuation characters and   \n",
    "            # add the stripped word to the token list\n",
    "        tokens.append(word.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(\"The man said: I've sat on the mat by the cat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'man', 'said', 'ive', 'sat', 'on', 'the', 'mat', 'by', 'the', 'cat']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 10.1.b\n",
    "\n",
    "Implement an `ngram` function that splits tokens into N-grams. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement a ngram functio that splits tokens into N-grams.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(tokens, n):\n",
    "    ngrams = []\n",
    "    # Create ngrams\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    grams = zip(*[tokens[i:] for i in range(n)])\n",
    "    ngrams.append([\" \".join(ngram) for ngram in grams])\n",
    "    return ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the man',\n",
       "  'man said',\n",
       "  'said ive',\n",
       "  'ive sat',\n",
       "  'sat on',\n",
       "  'on the',\n",
       "  'the mat',\n",
       "  'mat by',\n",
       "  'by the',\n",
       "  'the cat']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(tokens, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement an one-hot-encode function to create a numerical vector from a list of tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(tokens, num_words):\n",
    "    token_index = {}\n",
    "    # First, build an index of all tokens\n",
    "    for word in tokens:\n",
    "        if word not in token_index:\n",
    "            # Assign a unique index to each unique word\n",
    "            token_index[word] = len(token_index) + 1 # Note that we don't attribute index 0 to anything.\n",
    "    # vectorize the tokens\n",
    "    results = np.zeros((len(tokens), num_words, max(token_index.values()) + 1))\n",
    "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
    "        index = token_index.get(word)\n",
    "        results[j, index] = 1.\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       [[1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.],\n",
       "        [1., 1., 1., ..., 1., 1., 1.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encode(tokens, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 10.2:  Sequential Neural Network with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify text with a sequential model including embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = \"c:\\\\dev\\\\code\\\\DSC650\\\\dsc650\\\\data\\\\external\\\\imdb\\\\aclImdb\"\n",
    "\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "# collect reviews into a list of strings and collect review labels into a labels list\n",
    "# negative reviews stored in negative directory and positive reviews in positive directory\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            # https://stackoverflow.com/questions/9233027/unicodedecodeerror-charmap-codec-cant-decode-byte-x-in-position-y-character\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize the raw IMDB text data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100  # cut reviews after 100 words\n",
    "training_samples = 15000  \n",
    "validation_samples = 10000  \n",
    "max_words = 10000  # limit to the top 10,000 words in the dataset\n",
    "\n",
    "# tokenizing the text of the raw data\n",
    "# create vocabulary index based on word frequency\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# transform to a sequence of vectors\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# pad data b/c all sequences in a batch must be of same length\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# shuffle the data because samples were ordered by negative and positive\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "469/469 [==============================] - 9s 20ms/step - loss: 0.6942 - accuracy: 0.4955 - val_loss: 0.6931 - val_accuracy: 0.5025\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.4721 - accuracy: 0.7842 - val_loss: 0.8978 - val_accuracy: 0.5016\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0950 - accuracy: 0.9673 - val_loss: 1.4497 - val_accuracy: 0.4924\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0185 - accuracy: 0.9960 - val_loss: 1.7991 - val_accuracy: 0.4869\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 2.1311 - val_accuracy: 0.4927\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 2.7052 - val_accuracy: 0.4915\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0029 - accuracy: 0.9986 - val_loss: 2.9304 - val_accuracy: 0.4878\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 3.0612 - val_accuracy: 0.4875\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 8s 17ms/step - loss: 0.0025 - accuracy: 0.9983 - val_loss: 3.1649 - val_accuracy: 0.4902\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 9s 18ms/step - loss: 0.0030 - accuracy: 0.9985 - val_loss: 3.2275 - val_accuracy: 0.4876\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "# instantiiate a model\n",
    "model = Sequential()\n",
    "# add embedding layer to vectorize words\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "# flattens 3D embedded tensor into a 2D tensor\n",
    "model.add(Flatten())\n",
    "# binary classifier model\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# review layers\n",
    "model.summary()\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# fit the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize the test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(imdb_dir, 'test')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(test_dir, label_type)\n",
    "    for fname in sorted(os.listdir(dir_name)):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname), encoding=\"utf8\")\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "y_test = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evalute the model on the test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 2s 2ms/step - loss: 3.1824 - accuracy: 0.4983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.182436466217041, 0.49827998876571655]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Add LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and preprocess the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 4s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train shape: (25000, 500)\n",
      "input_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000  # number of words to consider as features\n",
    "maxlen = 500  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train2), (input_test, y_test2) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train), 'train sequences')\n",
    "print(len(input_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "# pad data b/c all sequences in a batch must be of same length\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train shape:', input_train.shape)\n",
    "print('input_test shape:', input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 45s 284ms/step - loss: 0.4946 - acc: 0.7672 - val_loss: 0.3568 - val_acc: 0.8518\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 44s 278ms/step - loss: 0.2869 - acc: 0.8885 - val_loss: 0.2855 - val_acc: 0.8792\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 49s 311ms/step - loss: 0.2306 - acc: 0.9113 - val_loss: 0.3750 - val_acc: 0.8628\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 51s 323ms/step - loss: 0.2009 - acc: 0.9241 - val_loss: 0.3156 - val_acc: 0.8834\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 51s 326ms/step - loss: 0.1779 - acc: 0.9365 - val_loss: 0.2826 - val_acc: 0.8870\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 51s 323ms/step - loss: 0.1579 - acc: 0.9437 - val_loss: 0.2961 - val_acc: 0.8782\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 52s 334ms/step - loss: 0.1460 - acc: 0.9467 - val_loss: 0.4033 - val_acc: 0.8766\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 53s 335ms/step - loss: 0.1327 - acc: 0.9517 - val_loss: 0.3442 - val_acc: 0.8650\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 48s 308ms/step - loss: 0.1228 - acc: 0.9582 - val_loss: 0.4502 - val_acc: 0.8628\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 53s 334ms/step - loss: 0.1148 - acc: 0.9607 - val_loss: 0.3598 - val_acc: 0.8554\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "# instantiiate a model\n",
    "model = Sequential()\n",
    "# add embedding layer to to vectorize words\n",
    "model.add(Embedding(max_features, 32))\n",
    "# add LSTM RNN layer\n",
    "model.add(LSTM(32))\n",
    "# binary classifier model\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "# fit the model\n",
    "history = model.fit(input_train, y_train2,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
